# 大模型基本概念

* `batch size`：每批训练数据的数量。
* `epoch`：一个完整的迭代过程，即一次完整的训练。
* `batch`：一组训练数据

# LORA微调

AdamW学习率（`learning_rate`）是3e-4，衰减率为（`weight_decay`）0.01；

最佳的SGD（内存占用较少）学习率是0.1，动量为0.9


```bash
# Hyperparameters
learning_rate = 3e-4
batch_size = 128
micro_batch_size = 1
max_iters = 50000  # train dataset size
weight_decay = 0.01
lora_r = 8
lora_alpha = 16
lora_dropout = 0.05
lora_query = True
lora_key = False
lora_value = True
lora_projection = False
lora_mlp = False
lora_head = False
warmup_steps = 100
```

LoRA超参数微调：增加R值

“r”是LoRA最重要的参数之一，它决定了LoRA矩阵的秩（rank）或维度（dimension），直接影响了模型的复杂度和容量。较高的“r”值意味着更强的表达能力，但可能导致过拟合；较低的“r”值可以减少过拟合，但代价是降低了表达能力。

LoRA超参数调优：更改Alpha

我们在保持LoRA的alpha参数不变的情况下，增加了矩阵秩r，较高的alpha更强调低秩结构或正则化，较低的alpha则减少了其影响，使模型更依赖于原始参数。调整“alpha”有助于在拟合数据和通过正则化防止过拟合之间保持平衡。一般来说，微调LLM时选择的alpha值是秩的两倍（请注意，当使用扩散模型时，情况可能不同）。

```bash
r=8
alpha=16
可训练参数：20277248个
不可训练参数：6738415616个
内存占用：16.42 GB

##### 推荐这个配置,效果稍微好一些
r=16
alha=32
可训练参数：40554496个
不可训练参数：6738415616个
内存占用：16.47 GB
```

我们还可以在查询权重矩阵、投影层、多头注意力模块之间的其他线性层以及输出层启用 LoRA，如果我们在这些附加层上加入 LoRA，那么对于 7B 的 Llama 2 模型，可训练参数的数量将从 4,194,304 增加到 20,277,248，增加五倍。在更多层应用 LoRA，能够显著提高模型性能，但也对内存空间的需求量更高。

`QLoRA`, 与 LoRA 方式类似，也是训练两个拟合参数层来达到对原始模型的调整。区别在于为了节省训练硬件资源， QLoRA 会先将原始模型参数量化至 4-bit 并冻结，然后添加一小组可学习的低秩适配器权重（ Low-rank Adapter weights），这些权重通过量化权重的反向传播梯度进行调优，在量化之后的参数上进行 LoRA 训练，这将大幅下降显存的占用（33b 的模型 以 FP16 全量加载需消耗 80GB 显存，量化至 4 bit之后模型加载仅需要 20 GB 左右显存的占用）。除了量化并冻结原始参数，QLoRA 还支持分页优化器：使用NVIDIA统一内存特性，将部分显存溢出的部分 offload 到内存中实现分页，来进一步避免 OOM 的问题。

## 重要事项

* 模型加速（vLLM）
* 数据集质量很重要（需要包含我们关心的所有任务的数据）
* 需要支持的任务类型比较多的时候（r）值可能需要设置的比较大
* `r`值和`epoch`值过大可能会导致训练过拟合
* Alpha值推荐设置成r值的两倍

# RAG

## 成功要求

* 检索必须能够找到与用户查询最相关的文档。
* 生成必须能够充分利用检索到的文档来足够回答用户的查询。

## 找到与用户查询最相关的文档

用户文档的存储要合理，不能直接保存原文切分之后的块向量。

* 块大小优化：由于LLMs受上下文长度限制，在构建外部知识数据库时需要对文档进行分块。太大或太小的块可能会导致生成组件出现问题，从而导致不准确的响应。
* 结构化外部知识：在复杂的场景中，可能需要比基本的向量索引更加结构化地构建外部知识，以便在处理合理分离的外部知识源时进行递归检索或路由检索。
* 如果需要从许多文档中检索信息，能够高效地在其中进行搜索，找到相关信息，并在单个答案中综合这些信息，并引用来源。在处理大型数据库时，一种高效的方法是创建两个索引（一个由摘要组成，另一个由文档片段组成），并分两步进行搜索，首先通过摘要筛选出相关文档，然后仅在这个相关组内部进行搜索。

## 充分利用检索到的文档

检索到的数据不要直接传给LLM进行推理，尽量优化重组之后再给LLM。

* 信息压缩：LLM不仅受上下文长度限制，而且如果检索到的文档包含太多噪音（即无关信息），可能会导致响应降级。
* 结果重新排名：LLM（大型语言模型）遭受所谓的“中间丢失”现象，即LLM倾向于关注提示的极端部分。基于此，有益的做法是在将检索到的文档传递给生成组件之前对其进行重新排名。

## 同时解决检索和生成成功要求的高级技术

* 生成器增强检索：这些技术利用LLM固有的推理能力，在执行检索之前，对用户查询进行细化，以更好地指示需要提供有用响应的内容。
* 迭代式检索生成器RAG：对于一些复杂情况，可能需要多步推理来提供对用户查询有用且相关的答案。
* 最终prompt发送给LLM生成回答：一是通过逐块发送检索到的上下文到LLM来迭代地完善答案；二是总结检索到的上下文以适应提示；三是基于不同的上下文块生成多个答案，然后将它们连接或总结起来。



# 常用模型记录

语音转文字：PaddleSpeech、FunAsr


