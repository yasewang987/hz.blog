# LORA微调

AdamW学习率是3e-4，衰减率为0.01；最佳的SGD学习率是0.1，动量为0.9

LoRA超参数微调：增加R值

“r”是LoRA最重要的参数之一，它决定了LoRA矩阵的秩（rank）或维度（dimension），直接影响了模型的复杂度和容量。较高的“r”值意味着更强的表达能力，但可能导致过拟合；较低的“r”值可以减少过拟合，但代价是降低了表达能力。

LoRA超参数调优：更改Alpha

我们在保持LoRA的alpha参数不变的情况下，增加了矩阵秩r，较高的alpha更强调低秩结构或正则化，较低的alpha则减少了其影响，使模型更依赖于原始参数。调整“alpha”有助于在拟合数据和通过正则化防止过拟合之间保持平衡。一般来说，微调LLM时选择的alpha值是秩的两倍（请注意，当使用扩散模型时，情况可能不同）。

当alpha值为秩的两倍（例如，r=256，alpha=512）时，的确产生了最佳结果

```text
r=8:
可训练参数：20277248个
不可训练参数：6738415616个
内存占用：16.42 GB

r=16:
可训练参数：40554496个
不可训练参数：6738415616个
内存占用：16.47 GB
```

# 模型加速

模型加速（vLLM）